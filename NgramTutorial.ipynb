{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPython Notebook - N-gram Tutorial\n",
    "\n",
    "*By Sam Ansari - Aug 13, 2015*\n",
    "\n",
    "First I'll see how far I can get with N-grams without outside resources\n",
    "\n",
    "We have a text file for [Pride and Prejudice from Project Gutenberg](https://www.gutenberg.org/ebooks/1342) stored as `pg1342.txt` in the same folder as our notebook. Let's load the text to a string since it's only 701KB, which will fit in memory nowadays. \n",
    "\n",
    "    *Note* : If we wanted to be more memory efficient we should parse the text file and store per word, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704149 , ï»¿The Project Gutenberg EBook of Pride and Prejud ...\n"
     ]
    }
   ],
   "source": [
    "with open('pg1342.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "# See the number of characters and the first 50 characters to confirm it is there    \n",
    "print len(txt), ',', txt[:50] , '...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now lets split into words into a big list, splitting on anything non-alphanumeric [A-Za-z0-9] (as well as punctuation) and forcing everything lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125897\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "words = re.split('[^A-Za-z]+', txt.lower())\n",
    "words = filter(None, words) # Remove empty strings\n",
    "\n",
    "# Print length of list\n",
    "print len(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets\n",
    "From this we can now generate N-grams, lets start with a 1-gram, basically the set of all the words\n",
    "\n",
    "    *Note* : One could use a dictionary instead of a set and keeping count of the occurances gives word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6528\n",
      "['foul', 'four', 'woods', 'hanging', 'woody', 'looking', 'eligible', 'scold', 'lord', 'meadows', 'sinking', 'leisurely', 'bringing', 'disturb', 'recollections', 'wednesday', 'piling', 'persisted', 'succession', 'tired']\n"
     ]
    }
   ],
   "source": [
    "import sets\n",
    "\n",
    "# Create set of all unique words, this throws away any information about frequency however\n",
    "gram1 = set(words)\n",
    "\n",
    "print len(gram1)\n",
    "\n",
    "# Instead of printing all the elements in the set, create an iterator and print 20 elements only\n",
    "gram1_iter = iter(gram1)\n",
    "print [gram1_iter.next() for i in xrange(20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try and get the 2-gram now, which is pairs of words. Let's have a quick look to see the last 10 and how they look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subscribe to\n",
      "to our\n",
      "our email\n",
      "email newsletter\n",
      "newsletter to\n",
      "to hear\n",
      "hear about\n",
      "about new\n",
      "new ebooks\n"
     ]
    }
   ],
   "source": [
    "# See the last 10 pairs\n",
    "for i in xrange(len(words)-10, len(words)-1):\n",
    "    print words[i], words[i+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, seems good, lets get all word pairs, and then generate a set of unique pairs from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125896\n",
      "55636\n",
      "[('her', 'taste'), ('every', 'kind'), ('five', 'shillings'), ('soothed', 'but'), ('seemed', 'most'), ('fortune', 'it'), ('of', 'thanking'), ('near', 'she'), ('understand', 'from'), ('it', 'looks'), ('have', 'made'), ('lucas', 'he'), ('fail', 'him'), ('new', 'to'), ('nothing', 'but'), ('fearful', 'on'), ('to', 'wander'), ('write', 'rather'), ('of', 'studying'), ('interruption', 'from')]\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [(words[i], words[i+1]) for i in xrange(len(words)-1)]\n",
    "print len(word_pairs)\n",
    "\n",
    "gram2 = set(word_pairs)\n",
    "print len(gram2)\n",
    "\n",
    "# Print 20 elements from gram2\n",
    "gram2_iter = iter(gram2)\n",
    "print [gram2_iter.next() for i in xrange(20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency\n",
    "\n",
    "Okay, that was fun, but this isn't enough, we need frequency if we want to have any sense of probabilities, which is what N-grams are about. Instead of using sets, lets create a dictionary with counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 4507), ('to', 4243), ('of', 3730), ('and', 3658), ('her', 2225), ('i', 2070), ('a', 2012), ('in', 1937), ('was', 1847), ('she', 1710), ('that', 1594), ('it', 1550), ('not', 1450), ('you', 1428), ('he', 1339), ('his', 1271), ('be', 1260), ('as', 1192), ('had', 1177), ('with', 1100)]\n"
     ]
    }
   ],
   "source": [
    "gram1 = dict()\n",
    "\n",
    "# Populate 1-gram dictionary\n",
    "for word in words:\n",
    "    if gram1.has_key(word):\n",
    "        gram1[word] += 1\n",
    "    else:\n",
    "        gram1[word] = 1 # Start a new entry with 1 count since saw it for the first time.\n",
    "\n",
    "# Turn into a list of (word, count) sorted by count from most to least\n",
    "gram1 = sorted(gram1.items(), key=lambda (word, count): -count)\n",
    "\n",
    "# Print top 20 most frequent words\n",
    "print gram1[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Pride and Prejudice, the words 'the', 'to', 'of', and 'and' were the top four most common words. Sounds about right, not too interesting yet, lets see what happens with 2-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('of', 'the'), 491), (('to', 'be'), 445), (('in', 'the'), 397), (('i', 'am'), 303), (('mr', 'darcy'), 273), (('to', 'the'), 268), (('of', 'her'), 261), (('it', 'was'), 251), (('of', 'his'), 235), (('she', 'was'), 212), (('she', 'had'), 205), (('had', 'been'), 200), (('it', 'is'), 194), (('i', 'have'), 188), (('to', 'her'), 179), (('that', 'he'), 177), (('could', 'not'), 167), (('he', 'had'), 166), (('and', 'the'), 165), (('for', 'the'), 163)]\n"
     ]
    }
   ],
   "source": [
    "gram2 = dict()\n",
    "\n",
    "# Populate 2-gram dictionary\n",
    "for i in xrange(len(words)-1):\n",
    "    key = (words[i], words[i+1])\n",
    "    if gram2.has_key(key):\n",
    "        gram2[key] += 1\n",
    "    else:\n",
    "        gram2[key] = 1\n",
    "\n",
    "# Turn into a list of (word, count) sorted by count from most to least\n",
    "gram2 = sorted(gram2.items(), key=lambda (_, count): -count)\n",
    "\n",
    "# Print top 20 most frequent words\n",
    "print gram2[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like `\"of the\"` and `\"to be\"` are the top two most common 2-grams, sounds good. \n",
    "\n",
    "## Next word prediction\n",
    "What can we do with this? Well lets see what happens if we take a random word from all the words, and build a sentence by just choosing the most common pair that has that word as it's start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enough\n"
     ]
    }
   ],
   "source": [
    "start_word = words[len(words)/4]\n",
    "print start_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just went ahead and chose the word that appears $1/4$ of the way into words, random enough.\n",
    "\n",
    "Now in a loop, iterate through the frequency list (most frequent first) and see if it matches the first word in a pair, if so, the next word is the second element in the word pair, and continue with that word. Stop after N words or the list does not contain that word.\n",
    "\n",
    "    *Note* : gram2 is a list that contains (key,value) where key is a word pair (first, second),\n",
    "             so you need element[0][0] for first word and element [0][1] for second word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start word: enough\n",
      "2-gram sentence: \" enough to be so much as to be so much as to be so much as to be so much \"\n"
     ]
    }
   ],
   "source": [
    "def get2GramSentence(word, n = 50):\n",
    "    for i in xrange(n):\n",
    "        print word,\n",
    "        # Find Next word\n",
    "        word = next((element[0][1] for element in gram2 if element[0][0] == word), None)\n",
    "        if not word:\n",
    "            break\n",
    "\n",
    "word = start_word\n",
    "print \"Start word: %s\" % word\n",
    "\n",
    "print \"2-gram sentence: \\\"\",\n",
    "get2GramSentence(word, 20)\n",
    "print \"\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets stuck in a loop pretty much straight away. Not very interesting, try out other words and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start word: and\n",
      "2-gram sentence: \" and the whole of the whole of the whole of the whole of the whole of the whole of the \"\n",
      "Start word: he\n",
      "2-gram sentence: \" he had been so much as to be so much as to be so much as to be so much \"\n",
      "Start word: she\n",
      "2-gram sentence: \" she was not be so much as to be so much as to be so much as to be so \"\n",
      "Start word: when\n",
      "2-gram sentence: \" when she was not be so much as to be so much as to be so much as to be \"\n",
      "Start word: john\n",
      "2-gram sentence: \" john with the whole of the whole of the whole of the whole of the whole of the whole of \"\n",
      "Start word: never\n",
      "2-gram sentence: \" never be so much as to be so much as to be so much as to be so much as \"\n",
      "Start word: i\n",
      "2-gram sentence: \" i am sure i am sure i am sure i am sure i am sure i am sure i am \"\n",
      "Start word: how\n",
      "2-gram sentence: \" how much as to be so much as to be so much as to be so much as to be \"\n"
     ]
    }
   ],
   "source": [
    "for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
    "    print \"Start word: %s\" % word\n",
    "\n",
    "    print \"2-gram sentence: \\\"\",\n",
    "    get2GramSentence(word, 20)\n",
    "    print \"\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted random choice based on frequency\n",
    "Same thing. Okay, lets randomly choose from the subset of all 2grams that matches the first word, using a weighted-probability based on counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def weighted_choice(choices):\n",
    "   total = sum(w for c, w in choices)\n",
    "   r = random.uniform(0, total)\n",
    "   upto = 0\n",
    "   for c, w in choices:\n",
    "      if upto + w > r:\n",
    "         return c\n",
    "      upto += w\n",
    "    \n",
    "def get2GramSentenceRandom(word, n = 50):\n",
    "    for i in xrange(n):\n",
    "        print word,\n",
    "        # Get all possible elements ((first word, second word), frequency)\n",
    "        choices = [element for element in gram2 if element[0][0] == word]\n",
    "        if not choices:\n",
    "            break\n",
    "        \n",
    "        # Choose a pair with weighted probability from the choice list\n",
    "        word = weighted_choice(choices)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start word: and\n",
      "2-gram sentence: \" and almost have allowed her model of her letter and goodness oh cried elizabeth this event of several years of \"\n",
      "Start word: he\n",
      "2-gram sentence: \" he chiefly in oh lizzy continued i could he had been really amiable light as well and within her brother \"\n",
      "Start word: she\n",
      "2-gram sentence: \" she is very gentleness as well as usual with this agreement shall never more have long and though it not \"\n",
      "Start word: when\n",
      "2-gram sentence: \" when he was not justify the world who are very kind from something was bestowed on looking up the loss \"\n",
      "Start word: john\n",
      "2-gram sentence: \" john with cold was first stage of dignity with the letter was convinced from much in the ball at all \"\n",
      "Start word: never\n",
      "2-gram sentence: \" never let me jane to her going there but i do not to the most likely that whatever his reserve \"\n",
      "Start word: i\n",
      "2-gram sentence: \" i had one of self destined for you must feel the project gutenberg volunteers and of an unpleasant sort of \"\n",
      "Start word: how\n",
      "2-gram sentence: \" how many weeks and loving them though it is tolerably till the neighbourhood it to equal to elizabeth could not \"\n"
     ]
    }
   ],
   "source": [
    "for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
    "    print \"Start word: %s\" % word\n",
    "\n",
    "    print \"2-gram sentence: \\\"\",\n",
    "    get2GramSentenceRandom(word, 20)\n",
    "    print \"\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that's way more interesting!** Those are starting to look like sentences!\n",
    "\n",
    "    *Note* It's pretty interesting to see that for the sentence \" when he believed him from the amiable but mrs hurst s being ill of being the discussion of course of \", we have hurst s, which we can tell came from Hurst's, an artifact of our stripping away all punctuation but keeping the s.\n",
    "\n",
    "Let's try a longer sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start word: it\n",
      "2-gram sentence: \" it would be the netherfield for whatever might see mr and show off two years had at dinner was banished and by her inquiries about it was unluckily for my own fault with an unforgiving speech they proposed being married and address that mr denny himself on mr bennet s \"\n"
     ]
    }
   ],
   "source": [
    "word = 'it'\n",
    "print \"Start word: %s\" % word\n",
    "print \"2-gram sentence: \\\"\",\n",
    "get2GramSentenceRandom(word, 50)\n",
    "print \"\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool, lets see what happens when we go to N-grams above 2.\n",
    "## Tri-grams and more\n",
    "Okay, let's create a Ngram generator that can let us make ngrams of arbitrary sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('i', 'do', 'not'), 62), (('i', 'am', 'sure'), 62), (('project', 'gutenberg', 'tm'), 57), (('as', 'soon', 'as'), 55), (('she', 'could', 'not'), 50), (('that', 'he', 'had'), 37), (('in', 'the', 'world'), 34), (('it', 'would', 'be'), 34), (('i', 'am', 'not'), 32), (('i', 'dare', 'say'), 31), (('the', 'project', 'gutenberg'), 31), (('could', 'not', 'be'), 30), (('it', 'was', 'not'), 30), (('that', 'he', 'was'), 29), (('mr', 'darcy', 's'), 29), (('that', 'it', 'was'), 28), (('on', 'the', 'subject'), 28), (('as', 'well', 'as'), 27), (('would', 'have', 'been'), 27), (('of', 'mr', 'darcy'), 27)]\n"
     ]
    }
   ],
   "source": [
    "def generateNgram(n=1):\n",
    "    gram = dict()\n",
    "    \n",
    "    # Some helpers to keep us crashing the PC for now\n",
    "    assert n > 0 and n < 20\n",
    "    \n",
    "    # Populate N-gram dictionary\n",
    "    for i in xrange(len(words)-(n-1)):\n",
    "        key = tuple(words[i:i+n])\n",
    "        if gram.has_key(key):\n",
    "            gram[key] += 1\n",
    "        else:\n",
    "            gram[key] = 1\n",
    "\n",
    "    # Turn into a list of (word, count) sorted by count from most to least\n",
    "    gram = sorted(gram.items(), key=lambda (_, count): -count)\n",
    "    return gram\n",
    "\n",
    "trigram = generateNgram(3)\n",
    "# Print top 20 most frequent ngrams\n",
    "print trigram[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Okay, let's see a selection of sentences for N-grams with N = 2 to 10 and a few starting words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 2-gram list... Done\n",
      "  2-gram: \" and opening in such an occasion of a woman must be mr darcy by a \"\n",
      "  2-gram: \" he was commissioning her exuberant spirits or next week she feels bingley was persuaded yourself \"\n",
      "  2-gram: \" she were over the river in acceding to be obliged to the room no stay \"\n",
      "  2-gram: \" when they had a table with louisa i was very confined for supposing his house \"\n",
      "  2-gram: \" john with other causes must be secure let me he i am determined if i \"\n",
      "  2-gram: \" never without anger could come mr darcy much satisfaction in the intelligence his inquiries after \"\n",
      "  2-gram: \" i do me you believe him and though the evening brought a disadvantage in his \"\n",
      "  2-gram: \" how many others then returned to know and bent on the address that you what \"\n",
      "\n",
      "Generating 3-gram list... Done\n",
      "  3-gram: \" and indeed but i have mortified mine believe him to see jane what is strong \"\n",
      "  3-gram: \" he chose to bear these two eldest daughter she could contain herself which his daughters \"\n",
      "  3-gram: \" she said he had often that they who had hoped it may arise chiefly for \"\n",
      "  3-gram: \" when you think she might be to welcome to be where where else elizabeth s \"\n",
      "  3-gram: \" john told me in a man chapter elizabeth suspected enough for i wonder said she \"\n",
      "  3-gram: \" never had been strange yes sir has been valued that he believed to step was \"\n",
      "  3-gram: \" i feel how to pratt for preparing to read the praise is a sort of \"\n",
      "  3-gram: \" how pleased to us i went on the table while these tastes had been able \"\n",
      "\n",
      "Generating 4-gram list... Done\n",
      "  4-gram: \" and generous good enough to see the carriage was most creditable gentlemanlike man you give \"\n",
      "  4-gram: \" he detained in he will believe not have been listened in your hearing this event \"\n",
      "  4-gram: \" she told me for continuing their pleasantest preservative she finds miss bennet will make his \"\n",
      "  4-gram: \" when mrs forster said miss bingley her affection sometimes one looked the morning mr wickham \"\n",
      "  4-gram: \" john told he could think she really too much for it on he wishes and \"\n",
      "  4-gram: \" never reach when persons whose mind and so steady candour is my daughters uncommonly fast \"\n",
      "  4-gram: \" i shall try to be finer success he has very little delicate a beautiful creature \"\n",
      "  4-gram: \" how mr bennet had been too full of his sisters may be urged the happy \"\n",
      "\n",
      "Generating 5-gram list... Done\n",
      "  5-gram: \" and of her a few weeks with the son unless you have nothing but however \"\n",
      "  5-gram: \" he now and herself said you never thought of associating with scarlet coat and trust \"\n",
      "  5-gram: \" she said lately learned some who is so much as this room she soon forgotten \"\n",
      "  5-gram: \" when you would have to their journey from the second time of all that she \"\n",
      "  5-gram: \" john told her for my life could be hurt if you must not sixpence of \"\n",
      "  5-gram: \" never see as to scotland but of my power and give you must have known \"\n",
      "  5-gram: \" i do it is over he was all walking if my mother followed he means \"\n",
      "  5-gram: \" how pleasant man quite comfortable on my youngest girl and are not till roused to \"\n",
      "\n",
      "Generating 6-gram list... Done\n",
      "  6-gram: \" and anxiety she stared many pleasant nature shall not thank me less agreeable his coming \"\n",
      "  6-gram: \" he offered to whom you will thank god s manners and to everybody was as \"\n",
      "  6-gram: \" she would be secure and kitty fretfully when that is all the project gutenberg tm \"\n",
      "  6-gram: \" when opposed their relationship to justify it is impossible not hear any rate there is \"\n",
      "  6-gram: \" john with an hour s going that scenes but then owned that she spoke and \"\n",
      "  6-gram: \" never been made himself in cried elizabeth was then shut the acutest kind as you \"\n",
      "  6-gram: \" i am sure said elizabeth was sure they are not and charitable donations in love \"\n",
      "  6-gram: \" how could not many circumstances of which nature had been at this desirable for you \"\n",
      "\n",
      "Generating 7-gram list... Done\n",
      "  7-gram: \" and the offer of advantage spent by his having promised to anybody s home and \"\n",
      "  7-gram: \" he had no answer and that all the end and where can know more cheerfully \"\n",
      "  7-gram: \" she sent them there was uppermost in that horrid man on either but to make \"\n",
      "  7-gram: \" when the best but though my aunt leaving wickham would have often disdained the sense \"\n",
      "  7-gram: \" john told him in mirth for additional cost and i speak with her daughters i \"\n",
      "  7-gram: \" never spoke to appear to give her turn her note aloud and such a period \"\n",
      "  7-gram: \" i must abominate writing because you have both must go on this subject of it \"\n",
      "  7-gram: \" how she will probably strike into her and was deserved but slightly surveying it is \"\n",
      "\n",
      "Generating 8-gram list... Done\n",
      "  8-gram: \" and though she was instantly on her surprise of the idea of peculiar vexation she \"\n",
      "  8-gram: \" he is that consoled her feel it taken leave to the advantage the survivor this \"\n",
      "  8-gram: \" she came from the effect on the case the table in town myself but as \"\n",
      "  8-gram: \" when sanctioned mr bingley and vexation to mrs bennet accepted but she had the gouldings \"\n",
      "  8-gram: \" john told his ankle in the kindness madam he would not affect concern which he \"\n",
      "  8-gram: \" never had mentioned earlier that if he had set forth into the mince pies for \"\n",
      "  8-gram: \" i could marry without forming any stay at his mind every glance at all the \"\n",
      "  8-gram: \" how very much of what to bring them and application was before they should be \"\n",
      "\n",
      "Generating 9-gram list... Done\n",
      "  9-gram: \" and help feeling towards the village when they had not but to me mr bennet \"\n",
      "  9-gram: \" he was the gentleman looking for i am sure she left the door jane in \"\n",
      "  9-gram: \" she had better for the circumstances to my dear replied jane was scarcely an expostulation \"\n",
      "  9-gram: \" when i must be long as marked her to eight o clock and from meryton \"\n",
      "  9-gram: \" john with a proper place which we may depend on every wish of thought it \"\n",
      "  9-gram: \" never heard mr darcy if you have actually persists in the course of fatigue and \"\n",
      "  9-gram: \" i ever induce me so indulgent as the whole of happiness in her progress was \"\n",
      "  9-gram: \" how very agreeable woman may elect to hear a relation lady catherine she ever since \"\n"
     ]
    }
   ],
   "source": [
    "def getNGramSentenceRandom(gram, word, n = 50):\n",
    "    for i in xrange(n):\n",
    "        print word,\n",
    "        # Get all possible elements ((first word, second word), frequency)\n",
    "        choices = [element for element in gram if element[0][0] == word]\n",
    "        if not choices:\n",
    "            break\n",
    "        \n",
    "        # Choose a pair with weighted probability from the choice list\n",
    "        word = weighted_choice(choices)[1]\n",
    "for n in xrange(2,10):\n",
    "    # Generate ngram list\n",
    "    print\n",
    "    print \"Generating %d-gram list...\" % n,\n",
    "    ngram = generateNgram(n)\n",
    "    print \"Done\"\n",
    "    \n",
    "    # Try out a bunch of sentences\n",
    "    for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
    "        print \"  %d-gram: \\\"\" % n,\n",
    "        getNGramSentenceRandom(ngram, word, 15)\n",
    "        print \"\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see the sentences getting better and better with larger n-grams, this correlates to the ngram having more foresight into the sentence structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 9-gram list... Done\n"
     ]
    }
   ],
   "source": [
    "# Generate 10gram list\n",
    "print\n",
    "print \"Generating %d-gram list...\" % n,\n",
    "gram10 = generateNgram(10)\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with the 10gram and see what sort of sentence comes out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9-gram: \" and she said her manner of whose return and i am married before what we may take his wife conducted yourselves so well as much when i entered the same and making her mother as for being hereafter her repeatedly to mrs bennet one of enjoying herself since we know it they are not be killed and once gone jane nor faulty degree of gratitude by letting her heart you so dull as follows having passed she asked her plan could not lady lucas whom nobody in our friend denny denied knowing forgive me against herself she will die of \"\n",
      "  9-gram: \" he supposed that colonel fitzwilliam s good brother is not be private care to ask when the morning within ten minutes before they knew myself sincerely i cannot always kept up any of her dear i dare say would not but could not till the matter cried when the country he had any such a family they had many pauses must have him before was meeting her daughters were directed her for he appeared again she could not plain chapter a intercourse a christian but haughty it when persons who does not have long he set amazed at table where \"\n",
      "  9-gram: \" she grew on miss bennet accepted a girl it gratified by your porridge and entreating her to return the means unprotected or obtain a capital added what i am not suit as quietly answered in rejecting him with him and the hand with you could not imagine it became perfect composure when mrs bennet looking farther northwards than either or having received soon but i have got up everything right to be by a very sorry however mrs phillips first points she hardly hold her letters you will influence a letter and though in hunsford every moment upon me to \"\n",
      "  9-gram: \" when she saw the lady was a project gutenberg tm trademark as good as may we had called economy was able to advantage of it is probably superior to me when the persuasion you are gratefully accepted the danger of a proper civilities bingley were standing and beg leave his kindness mr darcy thought only a very proud but this work of a letter was a little of his return mr darcy who seems he was on the hermitage elizabeth entered the regulars and his sisters to consider poetry as any place that my dressing gown before and miss bingley \"\n",
      "  9-gram: \" john with some prettier girls walked about it was distressed but at all that bingley with lizzy said they never yet for a little compassion to prove had ever the more contracted into the bride as they had most improbable event of the greatest satisfaction of all but without ceremony was rather a lively tone which she might expect such confusion said fitzwilliam without knowing that it may lead me to elizabeth when in company for i never to all others were going to make the honour of that he was not altered what is improved and mrs bennet scarcely \"\n",
      "  9-gram: \" never seen miss bingley standing on her situation remained therefore soon as before she affectionately taking you again the other warranties of not expect jane constantly so i am not help giving her ease with mutual desire of i had been was alone after an earnest i cannot fix on the point out as much grown girl but as soon satisfied and praise on meeting both sat with whom he i speak with no such behaviour equally ill opinion in elizabeth was forced to stay at rosings in without seeming really believe a companion added she began scolding her through \"\n",
      "  9-gram: \" i had such a wish to visit them with him accidentally in which shortly have been considered it is desired effect of their acquaintance still increasing and conciliatory manners and i know not afraid her heart you mean as to the motion of lydia what i must feel something of superior execution he was as well as he should stand by undervaluing their favourable than hurrying instantly understood that led by his dependence when they were to whom he dines here this for incivility though it will be as the united to write very respectable pleasures and elizabeth could call \"\n",
      "  9-gram: \" how shall be so soon as warmly but no connections their share of general habits that his gig and when their former my affection to eight o clock in which brought only to set off and pride he had resolved to attribute to be aware that i shall though she did mr bennet have a part i cannot last how sincerely i must require and was attentive to do not her anxious circumspection of her how could have been designed for him my aunt promised letter i believe me so imprudence forgotten there are not receiving so mildly to have \"\n"
     ]
    }
   ],
   "source": [
    "# Try out a bunch of sentences\n",
    "for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
    "    print \"  %d-gram: \\\"\" % n,\n",
    "    getNGramSentenceRandom(ngram, word, 100)\n",
    "    print \"\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks almost like normal sentences if you squint a little! Well, that was fun. Next up let's see some ways to improve upon this.\n",
    "\n",
    "Instead of just taking the next word every time, we could take the next k words etc.\n",
    "\n",
    "To be continue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
